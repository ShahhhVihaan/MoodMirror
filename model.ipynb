{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MoodMirror - Facial Emotion Detection\n",
    "\n",
    "In this notebook, we will train a deep learning model for facial emotion detection using the \"fer2013\" dataset from Kaggle. The model will classify facial emotions into seven categories: Angry, Disgust, Fear, Happy, Neutral, Sad, and Surprise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries and modules\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "from keras.callbacks import TensorBoard\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the TensorBoard callback\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We will define the image dimensions, set the batch size, and specify the paths for our training and validation datasets. We will also use Keras's ImageDataGenerator to apply real-time data augmentation to the images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Setting image dimensions and batch size\n",
    "IMG_HEIGHT = 48\n",
    "IMG_WIDTH = 48\n",
    "batch_size = 32\n",
    "\n",
    "# Paths for training and validation datasets\n",
    "train_data_dir = \"data/train/\"\n",
    "test_data_dir = \"data/test/\"\n",
    "\n",
    "# Creating ImageDataGenerators for training and validation datasets\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=30,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\",\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Loading training and validation data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    color_mode=\"grayscale\",\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    subset='training'\n",
    ")\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    color_mode=\"grayscale\",\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Training Data\n",
    "\n",
    "Let's visualize a random image from the training dataset along with its label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Displaying a random image from the training dataset\n",
    "class_labels = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Neutral\", \"Sad\", \"Surprise\"]\n",
    "img, label = train_generator.__next__()\n",
    "i = np.random.randint(0, img.shape[0])\n",
    "image = img[i]\n",
    "labl = class_labels[label[i].argmax()]\n",
    "plt.imshow(image[:, :, 0], cmap=\"gray\")\n",
    "plt.title(labl)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "We will define a sequential model with multiple convolutional layers, max-pooling layers, dropout layers, and dense layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 46, 46, 32)        320       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 44, 44, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 22, 22, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 22, 22, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 20, 20, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 10, 10, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10, 10, 128)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 4, 4, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               2097664   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 3591      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2489095 (9.50 MB)\n",
      "Trainable params: 2489095 (9.50 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Defining the model architecture\n",
    "model = Sequential()\n",
    "\n",
    "# Adding layers to the model\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\", input_shape=(48, 48, 1)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(7, activation=\"softmax\"))\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "We will train the model using the training data and validate it using the validation data. After training, we will save the model for future use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "897/897 [==============================] - 204s 226ms/step - loss: 1.7589 - accuracy: 0.2803 - val_loss: 1.6574 - val_accuracy: 0.3537\n",
      "Epoch 2/50\n",
      "897/897 [==============================] - 86s 96ms/step - loss: 1.6178 - accuracy: 0.3622 - val_loss: 1.4367 - val_accuracy: 0.4474\n",
      "Epoch 3/50\n",
      "897/897 [==============================] - 85s 95ms/step - loss: 1.4966 - accuracy: 0.4199 - val_loss: 1.3408 - val_accuracy: 0.4842\n",
      "Epoch 4/50\n",
      "897/897 [==============================] - 86s 96ms/step - loss: 1.4152 - accuracy: 0.4554 - val_loss: 1.2857 - val_accuracy: 0.4983\n",
      "Epoch 5/50\n",
      "897/897 [==============================] - 91s 101ms/step - loss: 1.3628 - accuracy: 0.4786 - val_loss: 1.2286 - val_accuracy: 0.5268\n",
      "Epoch 6/50\n",
      "897/897 [==============================] - 89s 100ms/step - loss: 1.3268 - accuracy: 0.4933 - val_loss: 1.2077 - val_accuracy: 0.5361\n",
      "Epoch 7/50\n",
      "897/897 [==============================] - 87s 97ms/step - loss: 1.2984 - accuracy: 0.4998 - val_loss: 1.2063 - val_accuracy: 0.5370\n",
      "Epoch 8/50\n",
      "897/897 [==============================] - 91s 101ms/step - loss: 1.2767 - accuracy: 0.5105 - val_loss: 1.1839 - val_accuracy: 0.5499\n",
      "Epoch 9/50\n",
      "897/897 [==============================] - 87s 97ms/step - loss: 1.2648 - accuracy: 0.5226 - val_loss: 1.1438 - val_accuracy: 0.5718\n",
      "Epoch 10/50\n",
      "897/897 [==============================] - 87s 97ms/step - loss: 1.2463 - accuracy: 0.5241 - val_loss: 1.1588 - val_accuracy: 0.5580\n",
      "Epoch 11/50\n",
      "469/897 [==============>...............] - ETA: 39s - loss: 1.2265 - accuracy: 0.5339"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\dpali\\Documents\\CSE 5717\\MoodMirror\\model.ipynb Cell 11\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dpali/Documents/CSE%205717/MoodMirror/model.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Training the model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dpali/Documents/CSE%205717/MoodMirror/model.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m50\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/dpali/Documents/CSE%205717/MoodMirror/model.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dpali/Documents/CSE%205717/MoodMirror/model.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     train_generator,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dpali/Documents/CSE%205717/MoodMirror/model.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49mnum_train_imgs \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m batch_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dpali/Documents/CSE%205717/MoodMirror/model.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dpali/Documents/CSE%205717/MoodMirror/model.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_generator,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dpali/Documents/CSE%205717/MoodMirror/model.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mnum_test_imgs \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m batch_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dpali/Documents/CSE%205717/MoodMirror/model.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[tensorboard_callback] \u001b[39m# Adding tensorboard callback here\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dpali/Documents/CSE%205717/MoodMirror/model.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dpali/Documents/CSE%205717/MoodMirror/model.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Saving the trained model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dpali/Documents/CSE%205717/MoodMirror/model.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39memotion_detection_model_100epochs.h5\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\engine\\training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1776\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1777\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1780\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1781\u001b[0m ):\n\u001b[0;32m   1782\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1783\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1784\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1785\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    828\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 831\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    833\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    834\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    864\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    865\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    866\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m   \u001b[39mreturn\u001b[39;00m tracing_compilation\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    868\u001b[0m       args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_config\n\u001b[0;32m    869\u001b[0m   )\n\u001b[0;32m    870\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    872\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39;49m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1260\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1261\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1262\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1263\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1264\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mflat_call(args)\n\u001b[0;32m   1265\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1266\u001b[0m     args,\n\u001b[0;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1268\u001b[0m     executing_eagerly)\n\u001b[0;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflat_call\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    216\u001b[0m   \u001b[39m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    218\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    251\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 252\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    253\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    254\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    255\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    256\u001b[0m     )\n\u001b[0;32m    257\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    259\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    260\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[0;32m    261\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[0;32m    262\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1477\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1478\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1479\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1480\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1481\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1482\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1483\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1484\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1485\u001b[0m   )\n\u001b[0;32m   1486\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1487\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1488\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1489\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1493\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1494\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[39m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   inputs \u001b[39m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m       tensor_conversion_registry\u001b[39m.\u001b[39mconvert(t)\n\u001b[0;32m     56\u001b[0m       \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, core_types\u001b[39m.\u001b[39mTensor)\n\u001b[0;32m     57\u001b[0m       \u001b[39melse\u001b[39;00m t\n\u001b[0;32m     58\u001b[0m       \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m inputs\n\u001b[0;32m     59\u001b[0m   ]\n\u001b[1;32m---> 60\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     63\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Calculating the number of images in training and test datasets\n",
    "num_train_imgs = sum([len(files) for r, d, files in os.walk(train_data_dir)])*.8\n",
    "num_val_imgs = sum([len(files) for r, d, files in os.walk(train_data_dir)])*.2\n",
    "\n",
    "# Training the model\n",
    "epochs = 20 # will be: 50\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=num_train_imgs // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=num_test_imgs // batch_size,\n",
    "    callbacks=[tensorboard_callback] # Adding tensorboard callback here\n",
    ")\n",
    "\n",
    "# Saving the trained model\n",
    "model.save(\"emotion_detection_model_100epochs.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Training Results\n",
    "\n",
    "We will plot the training and validation loss and accuracy over the epochs to visualize the performance of our model during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training and validation loss\n",
    "plt.plot(history.history[\"loss\"], \"y\", label=\"Training loss\")\n",
    "plt.plot(history.history[\"val_loss\"], \"r\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting training and validation accuracy\n",
    "plt.plot(history.history[\"accuracy\"], \"y\", label=\"Training accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], \"r\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model\n",
    "\n",
    "We will load the trained model and test its performance on a batch of validation images. We will also visualize the confusion matrix to see the accuracy of each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the trained model\n",
    "my_model = load_model(\"emotion_detection_model_100epochs.h5\", compile=False)\n",
    "\n",
    "# Making predictions on a batch of validation images\n",
    "test_img, test_lbl = validation_generator.__next__()\n",
    "predictions = my_model.predict(test_img)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "test_labels = np.argmax(test_lbl, axis=1)\n",
    "\n",
    "# Calculating and printing the accuracy\n",
    "print(\"Accuracy =\", metrics.accuracy_score(test_labels, predictions))\n",
    "\n",
    "# Displaying the confusion matrix\n",
    "cm = metrics.confusion_matrix(test_labels, predictions)\n",
    "sns.heatmap(cm, annot=True)\n",
    "\n",
    "# Displaying a random image from the validation dataset with its original and predicted labels\n",
    "n = np.random.randint(0, test_img.shape[0])\n",
    "image = test_img[n]\n",
    "orig_labl = class_labels[test_labels[n]]\n",
    "pred_labl = class_labels[predictions[n]]\n",
    "plt.imshow(image[:, :, 0], cmap=\"gray\")\n",
    "plt.title(f\"Original label: {orig_labl} | Predicted: {pred_labl}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We successfully trained a deep learning model for facial emotion detection using the \"fer2013\" dataset. The model can classify facial emotions into seven categories with reasonable accuracy. Further improvements can be made by fine-tuning the model, using a larger dataset, or employing more advanced architectures.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
